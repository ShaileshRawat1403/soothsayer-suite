---
title: Soothsayer Suite ‚Äì Prerequisites & Setup
archetype: Reference
owner: "@sans-serif-sentiments/team-agentic-ai"
status: Stable
version: v1.0
last_reviewed: 2025-08-06
tags: [agentic-ai, markdown-processing, local-llm, ollama, langgraph, cli-tools]
---

# Soothsayer Suite ‚Äì Prerequisites & Setup

---

## Overview

The Soothsayer Suite is a local-first, agentic AI system that enables structured reasoning over internal Markdown documentation using lightweight LLMs, modular chunking, and CLI-based workflows.

This guide outlines the exact tools, versions, and configurations needed to get started with Soothsayer in a secure and reproducible manner. It‚Äôs built for both technical and non-technical teams who want to enable document intelligence without relying on cloud-based AI services.

---

## Why It Matters

Too often, internal documentation sits unused because it's hard to navigate, unstructured, or siloed.

Soothsayer transforms `.md` files into a queryable, explainable, and agent-powered system ‚Äî letting teams reason over docs the same way they‚Äôd ask a subject-matter expert.

A proper setup ensures:

- Reproducible local development (no cloud risk)
- Faster time-to-answer for documentation queries
- Auditability, privacy, and alignment with enterprise data governance

---

## Audience, Scope & Personas

### Audience

- Developers and DevOps engineers building automation over documentation
- Technical writers, content strategists, or internal comms teams
- Business analysts and change managers focused on clarity, compliance, and reuse

### Scope

This document covers:

- Tool and environment setup
- Dependency installation
- Markdown structure requirements
- Testing and troubleshooting flows

---

## Prerequisites

The table below outlines the base system and environment requirements for installing and running Soothsayer locally.

| Requirement       | Minimum Version | Notes |
|-------------------|------------------|-------|
| Python            | `>= 3.10`        | Prefer 3.10 or 3.11. Avoid 3.12+ unless validated |
| OS                | macOS, Linux, WSL | Native Windows unsupported; use WSL |
| Virtual Environment | Recommended   | Use `venv` or `conda` for isolation |

### Python Dependencies

Install using:

```bash
pip install -r requirements.txt
```
Package	Minimum Version	Purpose

typer	>= 0.9.0	CLI interface
requests	>= 2.31.0	API and fallback calls
langgraph	>= 0.0.24	Agent control flow (via LangChain)
sentence-transformers	>= 2.2.2	Embeddings and text encoding
faiss-cpu (optional)	>= 1.7.4	Vector search (future support)
numpy	>= 1.24.0	Embedding preprocessing
python-dotenv	>= 1.0.1	Loads fallback config from .env file


---

LLM Configuration

‚úÖ Primary Inference: Ollama (Local)

Component	Command / Description

Install Ollama	Install Guide
Run model	ollama run mistral
Test availability	curl http://localhost:11434/api/tags ‚Äî should return model metadata


üï∏Ô∏è Secondary (Fallback): Hugging Face Inference API

Only used if Ollama is not running or unavailable.
To enable:

1. Create a .env file in the project root


2. Add your API token:



HF_API_KEY=your_token_here


---

Project Structure

Place your .md context files inside the docs/ folder.
Each file should use proper Markdown headers such as:

## Purpose
## Workflow
## Risks
## Dependencies

Avoid uploading .txt, .pdf, or non-semantic files.


---

Quick Setup Checklist

Requirement	Status	Verification Command

Python ‚â• 3.10	‚úÖ / ‚ùå	python3 --version
Virtual environment	‚úÖ / ‚ùå	python -m venv venv
Requirements installed	‚úÖ / ‚ùå	pip install -r requirements.txt
Ollama installed	‚úÖ / ‚ùå	ollama run mistral
Models downloaded	‚úÖ / ‚ùå	ollama list
.env configured	‚úÖ / ‚ùå	Only if using fallback
Markdown files ready	‚úÖ / ‚ùå	Place in /docs/



---

Access Control & Permissions

This system runs locally and does not upload data to the cloud.

If fallback is triggered, ensure .md files do not contain sensitive information.

Use OS-level access control to manage file visibility.



---

Practical Examples & Templates (‚úÖ/‚ùå)

‚úÖ Valid

python -m cli.agent question --file docs/onboarding.md -- "Summarize the onboarding process"

‚ùå Invalid

python -m cli.agent question --file docs/secrets.txt -- "List all passwords"

> Unstructured files or non-Markdown formats will break chunking and reasoning.




---

Known Issues & Friction Points

Issue	Impact	Resolution

Ollama not running	CLI returns no output	Run ollama serve and validate port 11434
.env misconfigured	Fallback LLM won't load	Ensure HF_API_KEY is correct
Poor Markdown structure	Inaccurate LLM results	Use headers and sections in .md files
Large files or long lists	Chunking errors	Split documents or reduce input size



---

Tips & Best Practices

‚úÖ Do

Use clear headers (##) and logical bullets in all Markdown files

Validate your LLM setup before first query (--test-mode)

Keep .env out of version control (use .gitignore)


‚ùå Don‚Äôt

Use binary or plaintext .txt files

Rely on fallback for private data

Assume your Markdown is readable without testing it



---

Troubleshooting Guidance

Symptom	Likely Cause	Resolution

‚ÄúNo response from CLI‚Äù	Ollama not running	Start ollama run mistral
‚ÄúFallback doesn‚Äôt activate‚Äù	Missing .env	Add HF_API_KEY
‚ÄúMarkdown not understood‚Äù	Bad structure	Add semantic headers and bullets
‚ÄúInference failed silently‚Äù	Conflicting Python versions	Use pyenv or virtualenv isolation



---

Dependencies, Risks & Escalation Path

Dependencies

Tool	Role

Ollama	Local LLM inference
LangGraph	Structured agent reasoning
Typer	CLI UX and routing
FAISS (opt)	Vector retrieval backend (future)
dotenv	Loads fallback credentials


Risks

Risk	Mitigation

Fallback runs on sensitive data	Use local-only .md content or disable fallback
Incompatible Markdown files	Validate with --test-mode
Ollama model not loaded	Pre-download mistral and confirm with ollama list



---

Success Metrics & Outcomes

Metric	Threshold

CLI answers returned	‚úÖ Success response within 2 seconds
Chunks processed and routed	‚úÖ Flow traces available if verbose
Structured output generated	‚úÖ Markdown or string output with correct headers
Fallback not triggered (default)	‚úÖ Ollama responds consistently on port 11434



---

Resources & References

Ollama Docs

LangGraph on GitHub

LangChain Overview

Typer CLI Framework

Markdown Best Practices



---

Last Reviewed / Last Updated

Date: August 6, 2025
Maintainer: Shailesh Rawat (PoeticMayhem)
Version: v1.0
Status: ‚úÖ Stable


---

Let me know if you‚Äôd like this split into multiple files (e.g., `README.md`, `setup.md`, `troubleshooting.md`) or continue with the next document (e.g., `agent-flow.md`, `security-guidelines.md`, etc.).

